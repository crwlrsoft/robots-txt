<p align="center"><a href="https://www.crwlr.software" target="_blank"><img src="https://github.com/crwlrsoft/graphics/blob/eee6cf48ee491b538d11b9acd7ee71fbcdbe3a09/crwlr-logo.png" alt="crwlr.software logo" width="260"></a></p>

# Robots Exclusion Standard/Protocol Parser
## for Web Crawling/Scraping

Use this library within crawler/scraper programs to parse robots.txt
files and check if your crawler user-agent is allowed to load certain
paths.

## Documentation
You can find the documentation at [crwlr.software](https://www.crwlr.software/packages/robots-txt/getting-started).

## Contributing

If you consider contributing something to this package, read the [contribution guide (CONTRIBUTING.md)](CONTRIBUTING.md).
